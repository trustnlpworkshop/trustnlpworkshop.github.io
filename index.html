<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>TrustNLP</title>

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="css/scrolling-nav.css" rel="stylesheet">

</head>

<body id="page-top">

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand js-scroll-trigger" href="#page-top">TrustNLP @ NAACL 2021</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#about">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#call_for_papers">Call for papers</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#people">People</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#program">Program</a>
          </li>
               <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#speakers">Speakers</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#contact">Contact</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <header class=" text-white" style="background-image: url('index.jpg')">
    <div class="container text-center">
      <h1>TrustNLP: First Workshop on Trustworthy Natural Language Processing</h1>
      <p class="desc">Colocated with the <a style="color:white" href="https://2021.naacl.org/"> Annual Conference of
          the North
          American
          Chapter of the Association for Computational Linguistics </a> </p>
      <p class="lead"></p>
    </div>
  </header>

  <section id="about">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <h2>About </h2>
          <p class="lead">Recent progress in Artificial Intelligence (AI) and Natural Language Processing (NLP) has greatly increased their presence in everyday consumer products in the last decade. Common examples include virtual assistants, recommendation systems, and personal healthcare management systems, among others. Advancements in these fields have historically been driven by the goal of improving model performance as measured by accuracy, but recently the NLP research community has started incorporating additional constraints to make sure models are fair and privacy-preserving. However, these constraints are not often considered together, which is important since there are critical questions at the intersection of these constraints such as the tension between simultaneously meeting privacy objectives and fairness objectives, which requires knowledge about the demographics a user belongs to. In this workshop, we aim to bring together these distinct yet closely related topics. </p>
        </div>
      </div>
    </div>
  </section>

  <section id="call_for_papers" class="bg-light">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <h2>Call for papers</h2>
          <p class="lead" {color: blue;}><b>Overview</b></p>
         <p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial;color:#212529;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">We&nbsp;invite papers which focus on developing models that are &ldquo;explainable, fair, privacy-preserving, causal, and robust&rdquo; (Trustworthy ML Initiative). Topics of interest include (but are not limited to):</span></p>
<ul>
    <li dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Differential Privacy</span></li>
    <li dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Fairness and Bias: Evaluation and Treatments</span></li>
    <li dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Model Explainability and Interpretability</span></li>
    <li dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Accountability</span></li>
     <li dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span
             style="font-size:12pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Ethics</span></li>
    <li dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Industry applications of Trustworthy NLP</span></li>
    <li dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Causal Inference</span></li>
    <li dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Secure and trustworthy data generation</span></li>
</ul>
<p class="lead"><b>Important Dates</b></p>
<ul>
    <li>
        <h3 dir="ltr" ><span
                style="font-size:12pt;font-family:Arial;color:#222222;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">March 29, 2021: Submission Date</span></h3>
    </li>
    <li>
        <h3 dir="ltr"><span style="font-size:12pt;font-family:Arial;color:#222222;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">April 15, 2021: Notification of Acceptance</span></h3>
    </li>
    <li>
        <h3 dir="ltr"><span style="font-size:12pt;font-family:Arial;color:#222222;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">April 26, 2021: Camera-ready papers due</span></h3>
    </li>
    <li>
        <h3 dir="ltr" ><span style="font-size:12pt;font-family:Arial;color:#222222;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">June 10, 2021: Workshop on Trustworthy NLP (TrustNLP)</span></h3>
    </li>
</ul>

          <p class="lead"><b>Submission Policy</b></p>
          <p>All submissions will be double-blind peer reviewed (with author names and affiliations removed) by the
              program committee and judged by their relevance to the workshop themes. </br> Accepted and under-review
                                                                                                papers are allowed to submit to the workshop.

Submitted manuscripts must be 8 pages long for full papers, and 4 pages long for short papers. Both full and short papers can have unlimited pages for references and appendices. Please note that at least one of the authors of each accepted paper must register for the workshop and present the paper.
Template files can be found here: https://www.overleaf.com/latex/templates/naacl-hlt-2021-latex-template/kvjhhyjsvmxf. </p>

<p>We also ask authors to include a broader impact and ethical concerns statement, following guidelines from the main
            conference.
</p>
            <p> Please submit to  <a href=" https://www.softconf.com/naacl2021/trustnlp2021/">https://www.softconf
                .com/naacl2021/trustnlp2021/ </a></p>

          <p class="lead"><b>Non-Archival option</b></p>
          <p>NAACL workshops are traditionally archival. To allow dual submission of work, we are also including a non-archival track. If accepted, these submissions will still participate and present their work in the workshop. A reference to the paper will be hosted on the workshop website (if desired), but will not be included in the official proceedings. Please submit through softconf but indicate that this is a cross submission at the bottom of the submission form. You can also skip this step and inform us of your non-archival preference after the reviews.</p>

          <p class="lead"><b>Anonymity Period</b></p>
          <p>We will follow NAACLâ€™s anonymity policy, and require full anonymity until time of acceptance (April 15, 2021). </p>

        </div>
      </div>
    </div>
  </section>
  
<section id="people">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <h2>People</h2>
 
         <p class="lead">Speakers</p>
	 <ul>
	  <li>  <a href="http://people.csail.mit.edu/korpusik/"> Mandy Korpusik</a> - Assistant professor, Loyola Marymount University </li>
	  <li>  <a href="http://www.cs.toronto.edu/~zemel/inquiry/home.php"> Richard Zemel</a> - Industrial Research Chair in Machine Learning, University of Toronto </li>
	  <li>  <a href="http://www.robertmunro.com/"> Robert Monarch</a> - Author, Human-in-the-Loop Machine Learning </li>
	 </ul>

          <p class="lead">Organizers</p>
	<ul>
	  <li>	  <a href="http://yadapruk.com"> Yada Pruksachatkun </a> - Alexa AI</li>
	  <li>    <a href="https://sail.usc.edu/~akramakr/"> Anil Ramakrishna </a> - Alexa AI</li>
	  <li>    <a href="http://web.cs.ucla.edu/~kwchang/"> Kai-Wei Chang </a> - UCLA, Amazon Visiting Academic</li>
  	  <li>    <a href="http://satyapriyakrishna.com/"> Satyapriya Krishna </a> - Alexa AI</li>
 	  <li>    <a href=""> Jwala Dhamala</a> - Alexa AI</li>
 	  <li>    <a href="https://tanayag.com/Home.html"> Tanaya Guha</a> - University of Warwick</li>
 	  <li>    <a href="http://ink-ron.usc.edu/xiangren/"> Xiang Ren</a> - USC</li>
	</ul>  	
          <p class="lead">Program committee</p>

	<ul>
	<li>    <a href="https://guptarah.github.io/"> Rahul Gupta </a>- Alexa AI</li>
	<li>    <a href="https://willieboag.wordpress.com/"> Willie Boag </a> - Massachusetts Institute of Technology</li>
	<li>    <a href="https://navkr.com/"> Naveen Kumar </a> - Disney Research</li>
	<li>    <a href="https://woollysocks.github.io/"> Nikita Nangia </a> - New York University</li>
	<li>    <a href="https://hhexiy.github.io/"> He He </a> - New York University</li>
	<li>    <a href="https://jyzhao.net/"> Jieyu Zhao </a> - University of California Los Angeles</li>
	<li>    <a href="https://vnpeng.net/"> Nanyun Peng </a> - University of California Los Angeles</li>
	<li>    <a href="https://www.amazon.science/author/spandana-gella"> Spandana Gella </a> - Alexa AI</li>
	<li>    <a href="https://moinnadeem.com/"> Moin Nadeem </a> - Massachusetts Institute of Technology</li>
	<li>    <a href="https://homes.cs.washington.edu/~msap/"> Maarten Sap </a> - University of Washington</li>
	<li>    <a href="http://www.cs.virginia.edu/~tw8cb/"> Tianlu Wang </a> - University of Virginia</li>
	<li>    <a href="https://sites.cs.ucsb.edu/~william/"> William Wang </a> - University of Santa Barbara</li>
	<li>    <a href="https://www.uvm.edu/cems/cs/profiles/joseph-near"> Joe Near </a> - University of Vermont</li>
	<li>    <a href="http://david.darais.com/"> David Darais </a> - Galois</li>
	<li>    Pratik Gajane - Department of Computer Science, Montanuniversitat Leoben, Austria</li>
	<li>    <a href="http://www.cs.cmu.edu/~pliang/"> Paul Pu Liang </a> - Carnegie Mellon University</li>
	<li>    <a href="https://u.cs.biu.ac.il/~gonenhi/"> Hila Gonen </a> - Bar-Ilan University</li>
	<li>    <a href="http://www.cs.toronto.edu/~pthaine/"> Patricia Thaine </a> - University of Toronto</li>
	<li>    <a href="https://jamiehay.es/"> Jamie Hayes </a> - Google DeepMind, University College London, UK</li>
	<li>    <a href="https://ewsheng.github.io/"> Emily Sheng </a> - University of California Los Angeles</li>
	<li>    <a href="https://isarnejad.github.io/"> Isar Nejadgholi</a> - National Research Council Canada</li>
	<li>    <a href="https://anthonyrios.net/"> Anthony Rios </a> - University of Texas at San Antonio</li>
	</ul>  	

        </div>
      </div>
    </div>
  </section>
 
<section id="program" class="bg-light">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <h2>Program</h2>
          <p class="lead">The tentative program (subject to change) is below</p>
            <p>&nbsp;</p>


<table>
<tbody>
<tr>
<td>
<p><span style="font-weight: 400;">Time</span></p>
</td>
<td>
<p><span style="font-weight: 400;">Event</span></p>
</td>
</tr>
<tr>
<td>
<p><span style="font-weight: 400;">9:00-9:10 am</span></p>
</td>
<td>
<p><span style="font-weight: 400;">Opening Address</span></p>
</td>
</tr>
<tr>
<td>
<p><span style="font-weight: 400;">9:10-50 am</span></p>
</td>
<td>
<p><span style="font-weight: 400;">Keynote 1: Richard Zemel</span></p>
</td>
</tr>
<tr>
<td>
<p><span style="font-weight: 400;">10-11 am</span></p>
</td>
<td>
<p><span style="font-weight: 400;">Paper Presentations</span></p>
</td>
</tr>
<tr>
<td>
<p><span style="font-weight: 400;">11-11:15 am</span></p>
</td>
<td>
<p><span style="font-weight: 400;">Break</span></p>
</td>
</tr>
<tr>
<td>
<p><span style="font-weight: 400;">11:15-12:15 pm</span></p>
</td>
<td>
<p><span style="font-weight: 400;">Paper Presentations</span></p>
</td>
</tr>
<tr>
<td>
<p><span style="font-weight: 400;">12:15-1:30 pm</span></p>
</td>
<td>
<p><span style="font-weight: 400;">Lunch break</span></p>
</td>
</tr>
<tr>
<td>
<p><span style="font-weight: 400;">1-2 pm</span></p>
</td>
<td>
<p><span style="font-weight: 400;">Mentorship Meeting</span></p>
</td>
</tr>
<tr>
<td>
<p><span style="font-weight: 400;">2-2:50 pm</span></p>
</td>
<td>
<p><span style="font-weight: 400;">Keynote 2: Mandy&nbsp;Korpusik</span></p>
</td>
</tr>
<tr>
<td>
<p><span style="font-weight: 400;">2:50-3 pm&nbsp;</span></p>
</td>
<td>
<p><span style="font-weight: 400;">Break</span></p>
</td>
</tr>
<tr>
<td>
<p><span style="font-weight: 400;">3-4 pm</span></p>
</td>
<td>
<p><span style="font-weight: 400;">Poster session</span></p>
</td>
</tr>
<tr>
<td>
<p><span style="font-weight: 400;">4:15 -5:05 pm</span></p>
</td>
<td>
<p><span style="font-weight: 400;">Keynote 3: Robert Munro</span></p>
</td>
</tr>
<tr>
<td>
<p><span style="font-weight: 400;">5:05-5:15 pm</span></p>
</td>
<td>
<p><span style="font-weight: 400;">Closing Address</span></p>
</td>
</tr>
</tbody>
</table>

  </section>



  <section id="speakers" class="bg-light">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <h2>Speakers</h2>
    <img src="robert_monarch_portrait.png" class="icons">

           <strong> <p style="text-align: center; font-size: 20px"> Measuring and Mitigating Bias in Training Data
</p></strong>
        <p  style="text-align: center;">Robert Munro,  </br>Author, Human-in-the-Loop Machine Learning</p>

    <p>Annotators are the largest and most diverse workforce in machine learning. They can teach the broader
        technical community much about inclusive participation for designing machine learning applications,
        especially when the annotators are subject matter experts building training data for applications that they
        will use. This talk covers three aspects of annotation: working with different annotation workforces;
        sampling data to improve diversity; and quality control for annotations when there are multiple subjective
        points-of-view. In each case, the talk will cover some common approaches to measuring and mitigating bias and the relative strengths and limitations of each approach.</p>

            <img src="mandy.jpg" class="icons">

           <strong> <p style="text-align: center; font-size: 20px">  Trustworthy Spoken Dialogue Systems: Application to Nutrition
</p></strong>
        <p  style="text-align: center;">Mandy Korpusik,  </br>Assistant Professor,Loyola Marymount University</p>

    <p>In this talk, I will give an overview of ethical issues concerning chatbots and spoken dialogue systems. Since
        neural generative models are trained on vast amounts of text from the internet, including Reddit, chatbots and dialogue systems may learn stereotypes and offensive language, such as the racist Tay chatbot deployed by Microsoft in 2016. As an application of dialogue systems for good, I will discuss a spoken diet tracking system to help with weight loss. In our work, deep learning techniques perform a semantic mapping from raw, unstructured, human natural language directly to a structured, relational database, without any intermediate pre-processing steps or string matching heuristics. Specifically, I will show that a novel, weakly supervised convolutional neural architecture learns a shared latent space, where vector representations of natural language queries lie close to embeddings of database entries that have semantically similar meanings. We are currently exploring personalized meal recommendations, computer vision for logging photos of food, a nutrition-specific speech recognizer, and exercise tracking. For future work on nutrition and fitness spoken dialogue systems, we will need to consider fairness and mitigate bias by ensuring food recommendations take into consideration each user's goals and dietary restrictions, expanding the food database to include other cuisines beyond American foods, and building a speech recognizer that works well for all accents.</p>

                <img src="rich.jpg" class="icons">
<strong> <p style="text-align: center; font-size: 20px">  Fairness and Invariant Learning
</p></strong>

        <p  style="text-align: center;">Richard Zemel,  </br>Industrial Research Chair in Machine Learning, University of Toronto</p>

            <p style="text-align: center;">   Robustness is of central importance in machine learning and has given rise
    to the fields of domain generalization and invariant learning, which are
    concerned with improving performance on a test distribution distinct from
    but related to the training distribution.  In this talk I will focus on
    links between research on invariant learning and algorithmic fairness and
    show how the two fields can be mutually beneficial.  While invariant
    learning methods typically rely on knowledge of disjoint domains or
    environments, sensitive label information indicating which demographic
    groups are at risk of discrimination is often used in the fairness
    literature.  Drawing inspiration from recent fairness approaches that
    improve worst-case performance without knowledge of sensitive groups, I will
    present a novel domain generalization method that handles the more realistic
    scenario where environment partitions are not provided. We will then see how
    this approach can outperform invariant-learning approaches with handcrafted
    environments in multiple cases. I will also describe how invariant learning
    methods can be applied to a fairnesss task, of predicting the toxicity of
    internet comments using the Civil Comments dataset. This work reveals
    potential benefits as well as limitations in the interaction between robust
    machine learning methods and algorithmic fairness.
</p>


        </div>
      </div>
    </div>
  </section>
  <section id="contact" >
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <h2>Contact us</h2>
          <p class="lead">  For questions, please contact us at trustnlpworkshoporganizers@gmail.com </p>
        </div>
      </div>
    </div>
  </section>
  <!-- Footer -->
  <footer class="py-5 bg-dark">
    <div class="container">
      <p class="m-0 text-center text-white">Copyright &copy; TrustNLP 2020</p>
    </div>
    <!-- /.container -->
  </footer>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Custom JavaScript for this theme -->
  <script src="js/scrolling-nav.js"></script>

</body>

</html>
