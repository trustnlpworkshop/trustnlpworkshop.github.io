<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>TrustNLP</title>

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="css/scrolling-nav.css" rel="stylesheet">

<style>
   table {border-collapse:collapse; table-layout:fixed; width:500px;}
   table td {border:solid 1px #fab; width:100px; word-wrap:break-word;}
   </style>

</head>

<body id="page-top">

<!-- Navigation -->
<nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand js-scroll-trigger" href="#page-top">TrustNLP @ NAACL 2024</a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#about">About</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#call-for-papers">Call for Papers</a>
        </li>          
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#speakers">Speakers</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#schedule">Schedule</a> 
        </li>          
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#papers">Papers</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#people">People</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#contact">Contact</a>
        </li>
      </ul>
    </div>
  </div>
</nav>

  <header class=" text-white" style="background-image: url('index.jpg')">
    <div class="container text-center">
      <h1>TrustNLP: Fourth Workshop on Trustworthy Natural Language Processing</h1>
      <p class="desc">Colocated with the <a style="color:white" href="https://2024.naacl.org/"> 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2024)</a> </p>
      <p class="lead"></p>
    </div>
  </header>

  <section id="about" class="bg-light">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <h2>About </h2>
          <p class="lead">Recent advances in Natural Language Processing, and the emergence of pretrained Large Language Models (LLM) specifically, have made NLP systems omnipresent in various aspects of our everyday life. In addition to traditional examples such as personal voice assistants, recommender systems, etc, more recent developments include content-generation models such as ChatGPT, text-to-image models (Dall-E), and so on. While these emergent technologies have an unquestionable potential to power various innovative NLP and AI applications, they also pose a number of challenges in terms of their safe and ethical use. To address such challenges, NLP researchers have formulated various objectives, e.g., intended to make models more fair, safe, and privacy-preserving. However, these objectives are often considered separately, which is a major limitation since it is often important to understand the interplay and/or tension between them. For instance, meeting a fairness objective might require access to users’ demographic information, which creates tension with privacy objectives. The goal of this workshop is to move toward a more comprehensive notion of Trustworthy NLP, by bringing together researchers working on those distinct yet related topics, as well as their intersection.
        </div>
      </div>
    </div>
  </section>



	
<section id="call-for-papers" class="bg-light">
  <div class="container">
    <div class="row">
      <!-- <div class="col-lg-8 mx-auto" tyle="margin-top: -150px;"> -->
        <div class="col-lg-8 mx-auto">

        <h2>Call for Papers</h2>
        <h4>Topics</h4>
        <p>We invite papers which focus on different aspects of <em>safe</em> and <em>trustworthy</em> language modeling. Topics of interest include (but are not limited to): 
        <ul style="padding-left: 20px;">
          <li>Secure, Faithful & Trustworthy Generation with LLMs</li>
          <li>Fairness in LLM alignment, Human Preference Elicitation, Participatory NLP</li>
          <li>Data Privacy Preservation and Data Leakage Issues in LLMs</li>
          <li>Toxic Language Detection and Mitigation</li>
          <li>Red-teaming, backdoor or adversarial attacks and defenses for LLM safety</li>
          <li>Explainability and Interpretability of LLM generation</li>
          <li>Robustness of LLMs</li>
          <li>Mitigating LLM Hallucinations & Misinformation</li>
          <li>Fairness and Bias in multi-modal generative models: Evaluation and Treatments</li>
          <li>Industry applications of Trustworthy NLP</li>
          <li>Trustworthy NLP challenges and opportunities for Latin American and Caribbean languages</li>
          <li>Regionally-relevant NLP fairness applications (toxicity, sentiment, content moderation, translation, etc.)</li>
        </ul>
        
        We welcome contributions which also draw upon interdisciplinary knowledge to advance Trustworthy NLP. This may include working with, synthesizing, or incorporating knowledge across expertise, sociopolitical systems, cultures, or norms.        
      </div>
    </div>
  </div>

  <div class="container", style="margin-top: 40px;">
    <div class="row">
      <div class="col-lg-8 mx-auto">
        <h4>Important Dates</h4>
        <ul style="padding-left: 20px;">

          <del><li><strong>Tues, April 2nd 2024:</strong> Workshop Paper Due Date (Direct Submission via <a href="https://softconf.com/naacl2024/TrustNLP2024/"> Softconf</a>)</li></del>
         <del><li><strong>Friday, April 5th, 2024:</strong> Workshop Paper Due Date (Fast-Track)</li></del>
         <del><li><strong>April 23rd, 2024:</strong> Notification of Acceptance </li></del>
         <del><li><strong>May 1st, 2024: </strong> Deadline for relevant NAACL Findings to submit non-archival (Direct submission via <a href="https://forms.gle/R3LWDCJK9V8PamfM7">this form</a>) </indent></del>
          <del><li><strong>May 3rd, 2024:</strong> Camera-ready Papers Due</li></del>
          <li><strong>Friday June 21, 2024:</strong> TrustNLP Workshop day</li>
      </ul>
      </div>
    </div>
  </div>

  <div class="container", style="margin-top: 20px;">
    <div class="row">
      <div class="col-lg-8 mx-auto">
          <div>
              <h4>Submission Information</h4>
              
              <div>
                <p>
                  All submissions undergo double-blind peer review (with author names and affiliations removed) by the
                  program committee, and they will be assessed based on their relevance to the workshop themes.
              </p>

                  <p>
                      All submissions go through the Softconf START conference management system. To submit, use <a href="https://softconf.com/naacl2024/TrustNLP2024/"> this Softconf submission link</a>.
                  </p>
                  
                  <p>
                      Submitted manuscripts must be 8 pages long for full papers and 4 pages long for short papers. Please
                      follow <a href="https://2024.naacl.org/calls/papers/#paper-submission-policies">NAACL submission
                          policies</a>. Both full and short papers can have unlimited pages for references and appendices.
                      Please note that at least one of the authors of each accepted paper must register for the workshop and
                      present the paper.
                      Template files can be found <a href="https://aclrollingreview.org/cfp#long-papers">here</a>.
                  </p>
                  <p>
                      We also ask authors to include a limitation section and broader impact statement, following guidelines
                      from the main conference.
                  </p>
              </div> 
          </div>
  
          <div style="margin-top: 10px;">
              <h5>Fast-Track Submission</h5>
              <p>
                  If your paper has been reviewed by ACL, EMNLP, EACL, or ARR and the average rating is higher than 2.5
                  (either average soundness or excitement score), the paper is qualified to be submitted to the fast-track.
                  In the appendix, please include the reviews and a short statement discussing what parts of the paper have
                  been revised.
              </p>
          </div>
  
          <div style="margin-top: 10px;">
              <h5>Non-Archival Option</h5>
              <p>
                  NAACL workshops are traditionally archival. To allow dual submission of work, we are also including a
                  non-archival track. If accepted, these submissions will still participate and present their work in the
                  workshop. A reference to the paper will be hosted on the workshop website (if desired), but will not be
                  included in the official proceedings. Please submit through  <a href="https://softconf.com/naacl2024/TrustNLP2024/"> Softconf</a> but indicate that this is a cross
                  submission at the bottom of the submission form. You can also skip this step and inform us of your
                  non-archival preference after the reviews. Papers accepted to the Findings of NAACL 2024 may also submit non-archival to the workshop <a href="https://forms.gle/R3LWDCJK9V8PamfM7">here</a>.
              </p>
          </div>
          </div>
  
          </div>
        </div>

  <div class="container", style="margin-top: 20px;">
    <div class="row">
      <div class="col-lg-8 mx-auto">
          <div>
              <h4>Policies</h4>

              <p>
                  Accepted and under-review papers are allowed to submit to the workshop but will not be included in the
                  proceedings.
              </p>
          </div>
  
              <p>
                  No anonymity period will be required for papers submitted to the workshop, per the latest updates to the
                  ACL anonymity policy. However, submissions must still remain fully anonymized.
              </p>
  
      </div>
  </div>
  </div>
  </section>
  

	






  <section id="speakers" class="bg-light">

    <div class="container">
      <div class="col-lg-8 mx-auto">   
        
        
        <div class="row">
          <h2>Invited Speakers</h2>        
          <div class="speaker">
            <div class="speaker-photo">
              <img src="speakers/maria.jpg" alt="Maria Pacheco">
            </div>          
            <h4>Maria Pacheco </h4>          
          <h5><strong>Assistant Professor, University of Colorado, Boulder</strong></h5>
            <p class="bio">
              <!-- <strong>Bio:</strong>  -->
              <a href="https://mlpacheco.github.io/">Maria Pacheco</a>  is an Assistant Professor in the Department of Computer Science and a Faculty Fellow in the Institute of Cognitive Science at the University of Colorado Boulder. Before joining CU, she was a Postdoctoral Researcher at Microsoft Research NYC. Maria completed her PhD in Computer Science at Purdue University, and her BSc. in Computer Science and Engineering at the Universidad Simon Bolivar in Caracas, Venezuela, where she was born and raised.
          </p>
          <h5><strong>Talk Title:</strong> Empowering text-as-data researchers with explainable NLP systems.</h5>
            
          <p class="talk">
            NLP systems are everywhere, and the broader research ecosystem is no exception. Researchers in the social sciences, humanities and industry have been making increasing use of NLP technologies to make sense of large textual repositories and answer fundamental questions in their fields of study. When NLP systems are employed as research tools, ensuring their reliability and transparency is incredibly important. Most current systems, while seemingly powerful, are opaque and often fail to satisfy the needs of researchers. In this talk, I will discuss the main challenges that arise when incorporating NLP technologies in text-as-data research, and make the case for explainable, controllable alternatives that can strike a balance between generalization and trustworthiness.
        </p>
      </div>  
      </div>
      <br>
      <div class="row">                              

        <div class="speaker">
          <div class="speaker-photo">
            <img src="speakers/prasanna.jpg" alt="Jieyu Zhao">
          </div>                   
          <h4>Prasanna Sattigeri</h4>
          <h5><strong>Principal Research Scientist, IBM Research</strong></h5>
            <p class="bio">
              <!-- <strong>Bio:</strong>  -->
              <a href="https://pronics2004.github.io/">Prasanna Sattigeri</a> is a Principal Research Scientist at IBM Research AI and the MIT-IBM Watson AI Lab, where his primary focus is on developing reliable AI solutions. His research interests encompass areas such as generative modeling, uncertainty quantification, and learning with limited data. His current projects are focused on the governance and safety of large language models (LLMs), aiming to establish both theoretical frameworks and practical systems that ensure these models are reliable and trustworthy. He has played a significant role in the development of several well-known open-source trustworthy AI toolkits, including AI Fairness 360, AI Explainability 360, and Uncertainty Quantification 360.
          </p>
          <p class="talk">
          <h5><strong>Talk Title:</strong> LLM Governance Elements: Detection and Alignment</h5>
        </p>This talk will go over the challenges and opportunities of developing and deploying large language models (LLMs), with a focus on building trustworthy AI systems. We investigate the difficulties of creating a detector library that can label a variety of risks, such as biased or hallucinated outputs. Detectors can be used not only as critical safeguards after deployment, but also for data curation and alignment during the development phase, allowing for effective governance across the LLM lifecycle. In addition, we will go over a few approaches that can help application developers tailor LLM behavior to not only mitigate common harms but also align with specific values or business requirements.  Finally, we emphasize the importance of human-centered model evaluation, discussing how explanations and source attribution can improve transparency and trust in LLM applications.

      </div> 
      </div>               
      <br>      
  <div class="row">    
                    <div class="speaker">
                      <div class="speaker-photo">
                        <img src="speakers/jieyu.jpg" alt="Jieyu Zhao">
                      </div>          
                      <h4>Jieyu Zhao </h4>          
                    <h5><strong>Assistant Professor, University of Southern California</strong></h5>
                      <p class="bio">
                        <!-- <strong>Bio:</strong>  -->
                        <a href="https://jyzhao.net">Jieyu Zhao</a> is an assistant professor of Computer Science Department at University of Southern California. Prior to that, she was an NSF Computing Innovation Fellow at University of Maryland, College Park. Jieyu received her Ph.D. from Computer Science Department, UCLA. Her research interest lies in fairness of ML/NLP models. Her paper got the EMNLP Best Long Paper Award (2017). She was one of the recipients of 2020 Microsoft PhD Fellowship and has been selected to participate in 2021 Rising Stars in EECS workshop. Her research has been covered by news media such as Wires, The Daily Mail and so on. She was invited by UN-WOMEN Beijing on a panel discussion about gender equality and social responsibility.
                    </p>
                    <h5><strong>Talk Title:</strong> Building Accountable NLP Models for Social Good</h5>
                      
                    <p class="talk">
                      The rapid advancement of natural language processing (NLP) technologies has unlocked a myriad of possibilities for positive societal impact, ranging from enhancing accessibility and communication to supporting disaster response and public health initiatives. However, the deployment of these technologies also raises critical concerns regarding accountability, fairness, transparency, and ethical use. In this talk, I will discuss  our efforts for auditing NLP models, detecting and mitigating biases, and understanding how LLMs make decisions. We hope to open the conversation to foster a community-wide effort towards more accountable and inclusive NLP practices.
                  </p>
                </div>  
                </div>
                <br>


              <div class="row">  
              <div class="speaker">
                <div class="speaker-photo">
                  <img src="speakers/ahmad.jpg" alt="Jieyu Zhao">
                </div>                 
                <h4>Ahmad Beirami</h4>
                <h5><strong>Research Scientist, Google Research</strong></h5>
                  <p class="bio"></p>
                    <!-- <strong>Bio:</strong>  -->
                    <a href="https://sites.google.com/view/beirami">Ahmad Beirami</a> is a research scientist at Google Research, co-leading  a research team on building safe, helpful, and scalable generative language models. At Meta AI, he led research to  power the next generation of virtual digital assistants with AR/VR capabilities through robust generative language modeling. At Electronic Arts, he led the AI agent research program for automated playtesting of video games and cooperative reinforcement learning. Before moving to industry in 2018, he held a joint postdoctoral fellow position at Harvard & MIT, focused on problems in the intersection of core machine learning and information theory. He is the recipient of the 2015 Sigma Xi Best PhD Thesis Award from Georgia Tech.
                </p>
                <h5><strong>Talk Title:</strong> Language Model Alignment: A Theoretical View</h5>
                <p class="talk">
                  The goal of the language model alignment (post-training) process is to draw samples from an aligned distribution that improves a reward (e.g., make the generation safer) but does not perturb much from the base model. A simple baseline for this task is best-of-N, where N responses are drawn from the base model, ranked based on a reward, and the highest ranking one is selected. More sophisticated techniques generally solve a KL-regularized reinforcement learning (RL) problem with the goal of maximizing expected reward subject to a KL divergence constraint between the aligned model and the base model. An alignment technique is preferred if its reward-KL tradeoff curve dominates other techniques. In this talk, we give an overview of language model alignment and give an understanding of known results in this space through simplified examples. We also present a new modular alignment technique, called controlled decoding, which solves the KL-regularized RL problem while keeping the base model frozen through learning a prefix scorer, offering inference-time configurability. Finally, we also shed light on the remarkable performance of best-of-N in terms of achieving competitive or even better reward-KL tradeoffs when compared to state-of-the-art alignment baselines.
              </p>
              </div>
            </div>  
            <br>

        </div>
    </div>
</section>




<section id="schedule" class="bg-light">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 mx-auto"> 
        
      <!-- </div> style="margin-top: -100px;"> -->
        <h2>Schedule </h2>
        <table>
          <tr>
            <th>Name</th>
            <th>Time (Mexico City Time, UTC−06:00)</th>
          </tr>
          <tr>
            <td>Opening remarks</td>
            <td>9:00 - 9:10am</td>
          </tr>
          <tr>
            <td><strong>Keynote 1</strong> (Maria Pacheco)</td>
            <td>9:10 - 9:50am</td>
          </tr>
          <tr>
            <td><strong>Keynote 2</strong> (Ahmad Beirami)</td>
            <td>9:50 - 10:30am</td>
          </tr>
          <tr>
            <td><strong>Virtual Poster Session + Coffee Break</strong></td>
            <td>10:30 - 11:10am</td>
          </tr>
          <tr>
            <td><strong>Keynote 3</strong> (Jieyu Zhao)</td>
            <td>11:10-11:50</td>
          </tr>
          <tr>
            <td><strong>Keynote 4</strong> (Prasanna Sattigeri)</td>
            <td>11:50am - 12:30pm</td>
          </tr>
          <tr>
            <td><strong>Lunch</strong></td>
            <td>12:30 - 2:00pm</td>
          </tr>
          <tr>
            <td><strong>In-person Poster Session</strong></td>
            <td>2:00 - 3:30pm</td>
          </tr>
          <tr>
            <td><strong>Coffee Break</strong></td>
            <td>3:30 - 4:00pm</td>
          </tr>
          <tr>
            <td><strong>Oral presentation / Best Paper Presentation</strong></td>
            <td>4:00-5:20pm</td>
          </tr>
          <tr>
            <td><strong>Closing Remarks</strong></td>
            <td>5:20-5:30pm</td>
          </tr>
        </table>
      </div>
    </div>
  </div>
</section>

  <section id="call_for_papers" class="bg-light">
  </section>

  <section id="papers">
    <div class="container">
      <div class="row">
      <div class="col-lg-8 mx-auto">
        <h2>Accepted Papers</h2>
        <h3>Archival Papers</h3>        

        <ul>
          <li>
            <a href="papers/1.pdf" target="_blank">Beyond Turing: A Comparative Analysis of Approaches for Detecting Machine-Generated Text</a>
            <br>
            <em>Muhammad Farid Adilazuarda</em>
          </li>
          <li>
            <strong>(Runner-up Best Long Paper) </strong><a href="papers/2.pdf" target="_blank">Automated Adversarial Discovery for Safety Classifiers</a>
            <br>
            <em>Yash Kumar Lal, Preethi Lahoti, Aradhana Sinha, Yao Qin and Ananth Balashankar</em>
          </li>
          <li>
            <a href="papers/3.pdf" target="_blank">FairBelief - Assessing Harmful Beliefs in Language Models</a>
            <br>
            <em>Mattia Setzu, Marta Marchiori Manerba, Pasquale Minervini and Debora Nozza</em>
          </li>
          <li>
            <a href="papers/4.pdf" target="_blank">The Trade-off between Performance, Efficiency, and Fairness in Adapter Modules for Text Classification</a>
            <br>
            <em>Minh Duc Bui and Katharina von der Wense</em>
          </li>
          <li>
            <a href="papers/5.pdf" target="_blank">When XGBoost Outperforms GPT-4 on Text Classification: A Case Study</a>
            <br>
            <em>Matyas Bohacek and Michal Bravansky</em>
          </li>
          <li>
            <a href="papers/7.pdf" target="_blank">Towards Healthy AI: Large Language Models Need Therapists Too</a>
            <br>
            <em>Baihan Lin, Djallel Bouneffouf, Guillermo Cecchi and Kush R. Varshney</em>
          </li>
          <li>
            <a href="papers/8.pdf" target="_blank">Exploring Causal Mechanisms for Machine Text Detection Methods</a>
            <br>
            <em>KiYoon Yoo, Wonhyuk Ahn, Yeji Song and Nojun Kwak</em>
          </li>
          <li>
            <a href="papers/11.pdf" target="_blank">FactAlign: Fact-Level Hallucination Detection and Classification Through Knowledge Graph Alignment</a>
            <br>
            <em>Mohamed Rashad, Ahmed Ismail Zahran, Abanoub Amgad Amin, Amr Yassin Abdelaal and Mohamed AlTantawy</em>
          </li>
          <li>
            <a href="papers/13.pdf" target="_blank">Cross-Task Defense: Instruction-Tuning LLMs for Content Safety</a>
            <br>
            <em>Yu Fu, Wen Xiao, Jia Chen, Jiachen Li, Evangelos Papalexakis, Aichi Chien and Yue Dong</em>
          </li>
          <li>
            <a href="papers/15.pdf" target="_blank">On the Interplay between Fairness and Explainability</a>
            <br>
            <em>Stephanie Brandl, Emanuele Bugliarello and Ilias Chalkidis</em>
          </li>
          <li>
            <a href="papers/18.pdf" target="_blank">Holistic Evaluation of Large Language Models: Assessing Robustness, Accuracy, and Toxicity for Real-World Applications</a>
            <br>
            <em>David Cecchini, Arshaan Nazir, Kalyan Chakravarthy and Veysel Kocaman</em>
          </li>
          <li>
            <strong>(Best Long Paper) </strong><a href="papers/19.pdf" target="_blank">HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation</a>
            <br>
            <em>Yihao Fang, Stephen W. Thomas and Xiaodan Zhu</em>
          </li>
          <li>
            <a href="papers/22.pdf" target="_blank">Overconfidence is Key: Verbalized Uncertainty Evaluation in Large Language and Vision-Language Models</a>
            <br>
            <em>Tobias Groot and Matias Valdenegro-Toro</em>
          </li>
          <li>
            <a href="papers/29.pdf" target="_blank">Tweak to Trust: Assessing the Reliability of Summarization Metrics in Contact Centers via Perturbed Summaries</a>
            <br>
            <em>Kevin Niranjanbhai Patel, Suraj Agrawal and Ayush Kumar</em>
          </li>
          <li>
            <strong>(Spotlight Paper) </strong><a href="papers/30.pdf" target="_blank">Flatness-Aware Gradient Descent for Safe Conversational AI</a>
            <br>
            <em>Leila Khalatbari, Saeid Hosseini, Hossein Sameti and Pascale Fung</em>
          </li>
          <li>
            <a href="papers/32.pdf" target="_blank">Introducing GenCeption for Multimodal LLM Benchmarking: You May Bypass Annotations</a>
            <br>
            <em>Lele Cao, Valentin Buchner, Zineb Senane and Fangkai Yang</em>
          </li>
          <li>
            <a href="papers/33.pdf" target="_blank">Semantic-Preserving Adversarial Example Attack against BERT</a>
            <br>
            <em>Chongyang Gao, Kang Gu, Soroush Vosoughi and Shagufta Mehnaz</em>
          </li>
          <li>
            <strong>(Spotlight Paper) </strong><a href="papers/35.pdf" target="_blank">Sandwich attack: Multi-language Mixture Adaptive Attack on LLMs</a>
            <br>
            <em>Bibek Upadhayay and Vahid Behzadan</em>
          </li>
          <li>
            <strong>(Spotlight Paper) </strong><a href="papers/36.pdf" target="_blank">Masking Latent Gender Knowledge for Debiasing Image Captioning</a>
            <br>
            <em>Fan Yang, Shalini Ghosh, Emre Barut, Kechen Qin, Prashan Wanigasekara, Chengwei Su, Weitong Ruan and Rahul Gupta</em>
          </li>
          <li>
            <a href="papers/38.pdf" target="_blank">BELIEVE: Belief-Enhanced Instruction Generation and Augmentation for Zero-Shot Bias Mitigation</a>
            <br>
            <em>Lisa Bauer, Ninareh Mehrabi, Palash Goyal, Kai-Wei Chang, Aram Galstyan and Rahul Gupta</em>
          </li>
          <li>
            <a href="papers/44.pdf" target="_blank">Tell Me Why: Explainable Public Health Fact-Checking with Large Language Models</a>
            <br>
            <em>Majid Zarharan, Pascal Wullschleger, Babak Behkam Kia, Mohammad Taher Pilehvar and Jennifer Foster</em>
          </li>
        </ul>


        <h3>Non-Archival Papers</h3>
        <ul>
          <li>Navigation as Attackers Wish? Towards Building Robust Embodied Agents under Federated Learning<br>
          <em>Yunchao Zhang, Zonglin Di, Kaiwen Zhou, Cihang Xie and Xin Eric Wang</em></li>
          <li>Uncertainty Assessment of Language Models through Rank-Calibration<br>
          <em>Xinmeng Huang, Shuo Li, Mengxin Yu, Matteo Sesia, Hamed Hassani, Insup Lee, Osbert Bastani and Edgar Dobriban</em></li>
          <li><strong>(Best Short Paper) </strong>White Men Lead, Black Women Help: Uncovering Gender, Racial, and Intersectional Bias in Language Agency<br>
          <em>Yixin Wan and Kai-Wei Chang</em></li>
          <li>ConsEval: Illuminating and Improving the Consistency of LLM Evaluators<br>
          <em>Jiwoo Hong and James Thorne</em></li>
          <li>Quantifying Memorization of Domain-Specific Pre-trained Language Models using Japanese Newspaper and Paywalls<br>
          <em>Shotaro Ishihara</em></li>
          <li>Beyond Visual Augmentation: Investigating Bias in Multi-Modal Text Generation<br>
          <em>Fnu Mohbat, Vijay Sadashivaiah, Keerthiram Murugesan, Amit Dhurandhar, Ronny Luss and Pin-Yu Chen</em></li>
          <li>Reevaluating Bias Detection in Language Models: The Role of Implicit Norms<br>
          <em>Farnaz Kohankhaki, Jacob-Junqi Tian, David B. Emerson, Laleh Seyyed-Kalantari and Faiza Khan Khattak</em></li>
          <li>BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models<br>
          <em>Chu Fei Luo, Ahmad Ghawanmeh, Xiaodan Zhu and Faiza Khan Khattak</em></li>
          <li><strong>(Runner-up Best Short Paper) </strong>Can Language Models Interpret Verbalized Uncertainty?<br>
          <em>Catarina Belem, Markelle Kelly, Sameer Singh, Mark Steyvers and Padhraic Smyth</em></li>
          <li><strong>(Spotlight Paper) </strong>CULTURE-GEN: Natural Language Prompts Reveal Uneven Culture Presence in Language Models<br>
          <em>Huihan Li, Liwei Jiang, Nouha Dziri, Xiang Ren and Yejin Choi</em></li>
          <li>Big Brother is Watching You: Automatically Jailbreak GPT-4V for Facial Recognition<br>
          <em>Yuanwei Wu, Yue Huang, Yixin Liu, Hanchi Sun, Xiang Li, Pan Zhou and Lichao Sun</em></li>
          <li>Selective "Selective Prediction": Reducing Unnecessary Abstention in Vision-Language Reasoning<br>
          <em>Tejas Srinivasan, Jack Hessel, Tanmay Gupta, Bill Yuchen Lin, Yejin Choi, Jesse Thomason and Khyathi Raghavi Chandu</em></li>
          <li>Evaluating Personal Information Parroting in Language Models<br>
          <em>Nishant Subramani, Kshitish Ghate and mona Diab</em></li>
          <li>Mitigating Social Biases in Language Models through Unlearning<br>
          <em>Omkar Dige, Diljot Singh, Tsz Fung Yau, Qixuan Zhang, Mohammad Bolandraftar, Xiaodan Zhu and Faiza Khan Khattak</em></li>
          <li>WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models<br>
          <em>Piotr Molenda, Adian Liusie and Mark Gales</em></li>
          <li>TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction<br>
          <em>Shuo Li, Sangdon Park, Insup Lee and Osbert Bastani</em></li>
          <li>Multi-Level Explanations for Generative Language Models<br>
          <em>Lucas Monteiro Paes, Dennis Wei, Hyo Jin Do, Hendrik Strobelt, Ronny Luss, Amit Dhurandhar, Manish Nagireddy, Karthikeyan Natesan Ramamurthy, Prasanna Sattigeri, Werner Geyer and Soumya Ghosh</em></li>
          <li>On the Calibration of Multilingual Question Answering LLMs<br>
          <em>Yahan Yang, Soham Dan, Dan Roth and Insup Lee</em></li>
          <li><strong>(Spotlight Paper) </strong>Chatgpt as an attack tool: Stealthy textual backdoor attack via blackbox generative model trigger<br>
          <em>Jiazhao Li, Yijin Yang, Zhuofeng Wu, V.G.Vinod Vydiswaran and Chaowei Xiao</em></li>
        </ul>
        
        <h3>Accepted NAACL Papers</h3>
        <ul>
          <li>From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning<br>
            <em>Xuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman Pan, Xiaoyang Wang, Ninghao Liu and Dong Yu</em></li>
            <li>Rationale- based Opinion Summarization<br>
            <em>Haoyuan Li, Snigdha Chaturvedi</em></li>
            <li>MisgenderMender: A Community-Informed Approach to Interventions for Misgendering<br>
            <em>Tamanna Hossain, Sunipa Dev, Sameer Singh</em></li> 
            <li>Tokenization Matters: Navigating Data-Scarce Tokenization for Gender Inclusive Language Technologies<br>
            <em>Anaelia Ovalle, Ninareh Mehrabi, Palash Goyal, Jwala Dhamala, Kai-Wei Chang, Richard Zemel, Aram Galstyan, Yuval Pinter, Rahul Gupta</em></li>

      </div>
    </div>
    </div>
  </section>

  <section id="people" class="bg-light">
    <div class="container">
      
      <div class="row">
      <div class="col-lg-8 mx-auto">
        
      <!-- </div>style="margin-top: -300px;"> -->
        

          <h2>Committee</h2>
          <p class="lead">Organizers</p>
          <ul>
            <li><a href="http://web.cs.ucla.edu/~kwchang/">Kai-Wei Chang</a> - UCLA, Amazon Visiting Academic</li>  
            <li><a href="https://anaeliaovalle.github.io/">Anaelia Ovalle</a> - UCLA</li>          
            <li><a href="http://ycao95.umiacs.io/">Yang Trista Cao</a> - University of Maryland</li>            
            <li><a href="http://scf.usc.edu/~ninarehm/">Ninareh Mehrabi</a> - Amazon Alexa AI</li>
            <li><a href="https://www.isi.edu/people/galstyan/about">Aram Galystan</a> - USC, Amazon Visiting Academic</li>
            <li><a href="https://jwaladhamala.com/">Jwala Dhamala</a> - Amazon Alexa AI</li>
            <li><a href="https://www.amazon.science/author/anoop-kumar">Anoop Kumar</a> - Capitol One</li>
            <li><a href="https://guptarah.github.io/">Rahul Gupta</a> - Amazon Alexa AI</li>
          </ul>
        
          
          <p class="lead">Program Committee</p>
          <ul>
            <li>Saied Alshahrani</li>
            <li>Connor Baumler</li>
            <li>Gagan Bhatia</li>
            <li>Keith Burghardt</li>
            <li>Yang Trista Cao</li>
            <li>Javier Carnerero Cano</li>
            <li>Canyu Chen</li>
            <li>Xinyue Chen</li>
            <li>Jwala Dhamala</li>
            <li>Árdís Elíasdóttir</li>
            <li>Aram Galstyan</li>
            <li>Usman Gohar</li>
            <li>Zihao He</li>
            <li>Pengfei He</li>
            <li>Qian Hu</li>
            <li>Satyapriya Krishna</li>
            <li>Jooyoung Lee</li>
            <li>Yanan Long</li>
            <li>Subho Majumdar</li>
            <li>Ninareh Mehrabi</li>
            <li>Sahil Mishra</li>
            <li>Isar Nejadgholi</li>
            <li>Huy Nghiem</li>
            <li>Anaelia Ovalle</li>
            <li>Jieyu Zhao</li>
            <li>Aishwarya Padmakumar</li>
            <li>Kartik Perisetla</li>
            <li>Salman Rahman</li>
            <li>Chahat Raj</li>
            <li>Anthony Rios</li>
            <li>Patricia Thaine</li>
            <li>Simon Yu</li>
            <li>Xinlin Zhuang</li>
            <li>Chupeng Zhang</li>
            <li>Chenyang Zhu</li>
            <li>Christina Chance</li>            
            <li>Nishant Balepur</li>                        
            <li>Elaine Yixin Wan</li>            
            <li>Xinchen Yang</li>                        
          </ul>
          
        </div>
      </div>
    </div>

    <div class="container", style="margin-top: 50px;">
      <div class="row">
        <div class="col-lg-8 mx-auto">
                <h5>Interested in reviewing for TrustNLP?</h5>
  
                <p>
                  If you are interested in reviewing submissions, please fill out this <a href="https://forms.gle/AHf3HvWMF3FVDh9q8">form</a>.


                </p>
    
        </div>
    </div>
    </div>

  </section>


  <section id="contact" class="bg-light">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto" style="margin-top: -100px;">
                      <h2>Questions?</h2>
        
                      <p>
                        Please contact us at <a href="mailto:trustnlp24naaclworkshop@googlegroups.com">trustnlp24naaclworkshop@googlegroups.com</a>.
      
      
                      </p>
          </div>
        </div>
      </div>
  </section>



  <section id="Sponsor" class="bg-light">
    <div class="container">
      <div class="col-lg-8 mx-auto">   

        <!-- <div class="row"> -->
          <h2>Thank you to our sponsors!</h2>        
          <div class="sponsor">
            <div class="sponsor-photo">
              <img src="sponsors/amazon-logo-1024x683.png">
      </div>
    </div>
    </section>





  <!-- Footer -->
  <footer class="py-5 bg-dark">
    <div class="container">
      <p class="m-0 text-center text-white">Copyright &copy; TrustNLP 2024</p>
    </div>
    <!-- /.container -->
  </footer>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Custom JavaScript for this theme -->
  <script src="js/scrolling-nav.js"></script>

</body>

</html>


            <!-- <div>

	<ul>

<li>Griffin Adams, Columbia University </li>
<li>Stefan Arnold, FAU Erlangen-Nurnberg</li>
<li>Connor Baumler, University of Maryland</li>
<li>Keith Burghardt, USC Information Sciences Institute</li>
<li>Yang Trista Cao, University of Maryland</li>
<li>Jwala Dhamala, Amazon Alexa AI-NLU</li>
<li>Jacob Eisenstein, Google</li>
<li>Katja Filippova, Google</li>
<li>Aram Galstyan, USC Information Sciences Institute</li>
<li>Umang Gupta, University of Southern California</li>
<li>Devamanyu Hazarika, Amazon</li>
<li>Zihao He, University of Southern California</li>
<li>William Held, Georgia Tech</li>
<li>Qian Hu, Amazon.com</li>
<li>Fatemah Husain, Kuwait University</li>
<li>Anoop Kumar, Amazon</li>
<li>Sasha Luccioni, Hugging Face</li>
<li>Pranav Narayanan Venkit, Pennsylvania State University</li>
<li>Isar Nejadgholi, National Research Council Canada</li>
<li>Aishwarya Padmakumar, Amazon</li>
<li>Ashwinee Panda, Princeton University</li>
<li>Anirudh Raju, Amazon, Alexa</li>
<li>Anthony Rios, University of Texas at San Antonio</li>
<li>Robik Shrestha, RIT</li>
<li>Anna Sotnikova, University of Maryland</li>
<li>Arjun Subramonian, University of California, Los Angeles</li>
<li>Jialu Wang, University of California, Santa Cruz</li>
<li>Chhavi Yadav, UCSD</li>
<li>Kiyoon Yoo, Seoul National University</li>

	</ul>  	
		</div> -->

  

        <!-- </div>
      </div>
    </div> -->
  <!-- </div> -->


  <!-- <section id="speakers" class="bg-light">
    <div class="container" style="display: none">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <h2>Speakers</h2>
            </br>
            <img src="Diyi_Yang.jpeg" class="icons">
            <strong> <p style="text-align: center; font-size: 20px"> Diyi Yang, Assistant Professor, School of Interactive Computing, Georgia Tech </p></strong>
            <p  style="text-align: center;">Diyi Yang is an assistant professor in the School of Interactive Computing at Georgia Tech. Diyi has broad interests in NLP and Computational Social Science, including dialogue summarization, limited data learning,  hate speech and bias,  as well as responsible NLP for mental health. Her work has received multiple awards (or nominations) at EMNLP, ICWSM, SIGCHI, and CSCW. She is a Microsoft Research Faculty Fellow, a Forbes 30 under 30 in Science, an IEEE “AI 10 to Watch”, a recipient of the Intel Rising Star Faculty Award, the Samsung AI Researcher of the Year and the NSF CAREER Award.</p>

            </br>
            <img src="sbmisi.jpeg" class="icons">
            <strong> <p  style="text-align: center;">Subho Majumdar, Senior Scientist, Splunk</p> </strong>
            <p style="text-align: center;"> Subho is a senior scientist in the Applied ML Research team of Splunk. Before recently joining Splunk, he spent 3 years in AT&T Data Science and AI Research (erstwhile part of AT&T Bell Labs). Throughout his career, he has worked on data-driven solutions that pushed the boundaries for a variety of challenging and cross-product problems. His current focus is on trustworthy machine learning in the wild: not only proposing novel solutions to technical problems that ensure qualities such as fairness, transparency, privacy, and robustness, but also implementing them in real-world use cases.</p>

            </br>
            <img src="Fei_Wang.jpeg" class="icons">
            <strong> <p  style="text-align: center;">Fei Wang, Associate Professor in Division of Health Informatics, Cornell University</p> </strong>
            <p style="text-align: center;">Fei Wang is an Associate Professor in Division of Health Informatics, Department of Population Health Sciences, Weill Cornell Medicine, Cornell University. His major research interest is data mining, machine learning and their applications in health data science. He has published more than 250 papers on the top venues of related areas such as ICML, KDD, NIPS, CVPR, AAAI, IJCAI, JAMA Internal Medicine, Annals of Internal Medicine, Lancet Digital Health, etc. His papers have received over 19,000 citations so far with an H-index 67. His (or his students’) papers have won 8 best paper (or nomination) awards at top international conferences on data mining and medical informatics. His team won the championship of the NIPS/Kaggle Challenge on Classification of Clinically Actionable Genetic Mutations in 2017 and Parkinson's Progression Markers' Initiative data challenge organized by Michael J. Fox Foundation in 2016. Dr. Wang is the recipient of the NSF CAREER Award in 2018, as well as the inaugural research leadership award in IEEE International Conference on Health Informatics (ICHI) 2019. Dr. Wang’s Research has been supported by NSF, NIH, ONR, PCORI, MJFF, AHA, Amazon, etc. Dr. Wang is the past chair of the Knowledge Discovery and Data Mining working group in American Medical Informatics Association (AMIA). Dr. Wang is a fellow of AMIA and a Distinguished Member of ACM.</p>
        </div>
      </div>
    </div>
  </section> -->



