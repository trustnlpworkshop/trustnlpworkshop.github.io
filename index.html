<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>TrustNLP</title>

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="css/scrolling-nav.css" rel="stylesheet">

<style>
   table {border-collapse:collapse; table-layout:fixed; width:500px;}
   table td {border:solid 1px #fab; width:100px; word-wrap:break-word;}
   </style>

</head>

<body id="page-top">

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand js-scroll-trigger" href="#page-top">TrustNLP @ NAACL 2024</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#about">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#call_for_papers">Call for papers</a>
          </li>          
	 <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#talk">Speakers</a>
          </li>
        <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#schedule">Schedule</a>
        </li>          
	 <!-- <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#papers">Papers</a>
          </li> -->

          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#people">People</a>
          </li>
         <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#contact">Contact</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <header class=" text-white" style="background-image: url('index.jpg')">
    <div class="container text-center">
      <h1>TrustNLP: Fourth Workshop on Trustworthy Natural Language Processing</h1>
      <p class="desc">Colocated with the <a style="color:white" href="https://2024.naacl.org/"> 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2024)</a> </p>
      <p class="lead"></p>
    </div>
  </header>

  <section id="about" class="bg-light">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <h2>About </h2>
          <p class="lead">Recent advances in Natural Language Processing, and the emergence of pretrained Large Language Models (LLM) specifically, have made NLP systems omnipresent in various aspects of our everyday life. In addition to traditional examples such as personal voice assistants, recommender systems, etc, more recent developments include content-generation models such as ChatGPT, text-to-image models (Dall-E), and so on. While these emergent technologies have an unquestionable potential to power various innovative NLP and AI applications, they also pose a number of challenges in terms of their safe and ethical use. To address such challenges, NLP researchers have formulated various objectives, e.g., intended to make models more fair, safe, and privacy-preserving. However, these objectives are often considered separately, which is a major limitation since it is often important to understand the interplay and/or tension between them. For instance, meeting a fairness objective might require access to users’ demographic information, which creates tension with privacy objectives. The goal of this workshop is to move toward a more comprehensive notion of Trustworthy NLP, by bringing together researchers working on those distinct yet related topics, as well as their intersection.
        </div>
      </div>
    </div>
  </section>





	
<section id="call_for_papers" class="bg-light">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 mx-auto" style="margin-top: -150px;">
        <h2>Call for Papers</h2>
        <h4>Topics</h4>
        <p>We invite papers which focus on different aspects of <em>safe</em> and <em>trustworthy</em> language modeling. Topics of interest include (but are not limited to): 
        <ul style="padding-left: 20px;">
          <li>Secure, Faithful & Trustworthy Generation with LLMs</li>
          <li>Fairness in LLM alignment, Human Preference Elicitation, Participatory NLP</li>
          <li>Data Privacy Preservation and Data Leakage Issues in LLMs</li>
          <li>Toxic Language Detection and Mitigation</li>
          <li>Red-teaming, backdoor or adversarial attacks and defenses for LLM safety</li>
          <li>Explainability and Interpretability of LLM generation</li>
          <li>Robustness of LLMs</li>
          <li>Mitigating LLM Hallucinations & Misinformation</li>
          <li>Fairness and Bias in multi-modal generative models: Evaluation and Treatments</li>
          <li>Industry applications of Trustworthy NLP</li>
          <li>Trustworthy NLP challenges and opportunities for Latin American and Caribbean languages</li>
          <li>Regionally-relevant NLP fairness applications (toxicity, sentiment, content moderation, translation, etc.)</li>
        </ul>
        
        We welcome contributions which also draw upon interdisciplinary knowledge to advance Trustworthy NLP. This may include working with, synthesizing, or incorporating knowledge across expertise, sociopolitical systems, cultures, or norms.        
      </div>
    </div>
  </div>


  <div class="container", style="margin-top: 40px;">
    <div class="row">
      <div class="col-lg-8 mx-auto">
        <h4>Important Dates</h4>
        <ul style="padding-left: 20px;">

          <del><li><strong>Tues, April 2nd 2024:</strong> Workshop Paper Due Date (Direct Submission via <a href="https://softconf.com/naacl2024/TrustNLP2024/"> Softconf</a>)</li></del>
         <del><li><strong>Friday, April 5th, 2024:</strong> Workshop Paper Due Date (Fast-Track)</li></del>
         <del><li><strong>April 23rd, 2024:</strong> Notification of Acceptance </li></del>
         <li><strong>May 1st, 2024: </strong> Deadline for relevant NAACL Findings to submit non-archival (Direct submission via <a href="https://forms.gle/R3LWDCJK9V8PamfM7">this form</a>) </indent>
          <li><strong>May 3rd, 2024:</strong> Camera-ready Papers Due</li>
          <li><strong>June 21, 2024:</strong> TrustNLP Workshop day</li>
      </ul>
      </div>
    </div>
  </div>

  <div class="container", style="margin-top: 20px;">
    <div class="row">
      <div class="col-lg-8 mx-auto">
          <div>
              <h4>Submission Information</h4>
              
              <div>
                <p>
                  All submissions undergo double-blind peer review (with author names and affiliations removed) by the
                  program committee, and they will be assessed based on their relevance to the workshop themes.
              </p>

                  <p>
                      All submissions go through the Softconf START conference management system. To submit, use <a href="https://softconf.com/naacl2024/TrustNLP2024/"> this Softconf submission link</a>.
                  </p>
                  
                  <p>
                      Submitted manuscripts must be 8 pages long for full papers and 4 pages long for short papers. Please
                      follow <a href="https://2024.naacl.org/calls/papers/#paper-submission-policies">NAACL submission
                          policies</a>. Both full and short papers can have unlimited pages for references and appendices.
                      Please note that at least one of the authors of each accepted paper must register for the workshop and
                      present the paper.
                      Template files can be found <a href="https://aclrollingreview.org/cfp#long-papers">here</a>.
                  </p>
                  <p>
                      We also ask authors to include a limitation section and broader impact statement, following guidelines
                      from the main conference.
                  </p>
              </div> 
          </div>
  
          <div style="margin-top: 10px;">
              <h5>Fast-Track Submission</h5>
              <p>
                  If your paper has been reviewed by ACL, EMNLP, EACL, or ARR and the average rating is higher than 2.5
                  (either average soundness or excitement score), the paper is qualified to be submitted to the fast-track.
                  In the appendix, please include the reviews and a short statement discussing what parts of the paper have
                  been revised.
              </p>
          </div>
  
          <div style="margin-top: 10px;">
              <h5>Non-Archival Option</h5>
              <p>
                  NAACL workshops are traditionally archival. To allow dual submission of work, we are also including a
                  non-archival track. If accepted, these submissions will still participate and present their work in the
                  workshop. A reference to the paper will be hosted on the workshop website (if desired), but will not be
                  included in the official proceedings. Please submit through  <a href="https://softconf.com/naacl2024/TrustNLP2024/"> Softconf</a> but indicate that this is a cross
                  submission at the bottom of the submission form. You can also skip this step and inform us of your
                  non-archival preference after the reviews. Papers accepted to the Findings of NAACL 2024 may also submit non-archival to the workshop <a href="https://forms.gle/R3LWDCJK9V8PamfM7">here</a>.
              </p>
          </div>
          </div>
  
          </div>
        </div>

  <div class="container", style="margin-top: 20px;">
    <div class="row">
      <div class="col-lg-8 mx-auto">
          <div>
              <h4>Policies</h4>

              <p>
                  Accepted and under-review papers are allowed to submit to the workshop but will not be included in the
                  proceedings.
              </p>
          </div>
  
              <p>
                  No anonymity period will be required for papers submitted to the workshop, per the latest updates to the
                  ACL anonymity policy. However, submissions must still remain fully anonymized.
              </p>
  
      </div>
  </div>
  </div>
  </section>
  

	





        </div>
      </div>
    </div>
  </section>

   <style>
    .speaker-photo {
      width: 150px;
      height: 150px;
      border-radius: 50%;
      overflow: hidden;
      margin-bottom: 20px;
    }
   
    .speaker-photo img {
      width: 100%;
      height: 100%;
      object-fit: cover;
    }
   </style>

  <section id="talk" class="bg-light">
    <div class="container">
      <div class="col-lg-8 mx-auto">   
        
        
        <div class="row">
          <h2>Invited Speakers</h2>        

                    <div class="speaker">
                      <div class="speaker-photo">
                        <img src="speakers/jieyu.jpg" alt="Jieyu Zhao">
                      </div>          
                      <h4>Jieyu Zhao </h4>          
                    <h5><strong>Assistant Professor, University of Southern California</strong></h5>
                      <p class="bio">
                        <!-- <strong>Bio:</strong>  -->
                        <a href="https://jyzhao.net">Jieyu Zhao</a> is an assistant professor of Computer Science Department at University of Southern California. Prior to that, she was an NSF Computing Innovation Fellow at University of Maryland, College Park. Jieyu received her Ph.D. from Computer Science Department, UCLA. Her research interest lies in fairness of ML/NLP models. Her paper got the EMNLP Best Long Paper Award (2017). She was one of the recipients of 2020 Microsoft PhD Fellowship and has been selected to participate in 2021 Rising Stars in EECS workshop. Her research has been covered by news media such as Wires, The Daily Mail and so on. She was invited by UN-WOMEN Beijing on a panel discussion about gender equality and social responsibility.
                    </p>
                    <h5><strong>Talk Title:</strong> Building Accountable NLP Models for Social Good</h5>
                      
                    <p class="talk">
                      The rapid advancement of natural language processing (NLP) technologies has unlocked a myriad of possibilities for positive societal impact, ranging from enhancing accessibility and communication to supporting disaster response and public health initiatives. However, the deployment of these technologies also raises critical concerns regarding accountability, fairness, transparency, and ethical use. In this talk, I will discuss  our efforts for auditing NLP models, detecting and mitigating biases, and understanding how LLMs make decisions. We hope to open the conversation to foster a community-wide effort towards more accountable and inclusive NLP practices.
                  </p>
                </div>  
                </div>
                <br>
            <div class="row">                              

                <div class="speaker">
                  <div class="speaker-photo">
                    <img src="speakers/prasanna.jpg" alt="Jieyu Zhao">
                  </div>                   
                  <h4>Prasanna Sattigeri</h4>
                  <h5><strong>Principal Research Scientist, IBM Research</strong></h5>
                    <p class="bio">
                      <!-- <strong>Bio:</strong>  -->
                      <a href="https://pronics2004.github.io/">Prasanna Sattigeri</a> is a Principal Research Scientist at IBM Research AI and the MIT-IBM Watson AI Lab, where his primary focus is on developing reliable AI solutions. His research interests encompass areas such as generative modeling, uncertainty quantification, and learning with limited data. His current projects are focused on the governance and safety of large language models (LLMs), aiming to establish both theoretical frameworks and practical systems that ensure these models are reliable and trustworthy. He has played a significant role in the development of several well-known open-source trustworthy AI toolkits, including AI Fairness 360, AI Explainability 360, and Uncertainty Quantification 360.
                  </p>
                  <p class="talk">
                  <h5><strong>Talk Title:</strong> TBD </h5>
                </p>

              </div> 
              </div>               
              <br>

              <div class="row">  
              <div class="speaker">
                <div class="speaker-photo">
                  <img src="speakers/ahmad.jpg" alt="Jieyu Zhao">
                </div>                 
                <h4>Ahmad Beirami</h4>
                <h5><strong>Research Scientist, Google Research</strong></h5>
                  <p class="bio"></p>
                    <!-- <strong>Bio:</strong>  -->
                    <a href="https://sites.google.com/view/beirami">Ahmad Beirami</a> is a research scientist at Google Research, co-leading  a research team on building safe, helpful, and scalable generative language models. At Meta AI, he led research to  power the next generation of virtual digital assistants with AR/VR capabilities through robust generative language modeling. At Electronic Arts, he led the AI agent research program for automated playtesting of video games and cooperative reinforcement learning. Before moving to industry in 2018, he held a joint postdoctoral fellow position at Harvard & MIT, focused on problems in the intersection of core machine learning and information theory. He is the recipient of the 2015 Sigma Xi Best PhD Thesis Award from Georgia Tech.
                </p>
                <h5><strong>Talk Title:</strong> TBD </h5>
              </div>
            </div>  
            <br>

        </div>
    </div>
</section>

  <section id="schedule" class="bg-light">
	    
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto" style="margin-top: -100px;">
          <h2>Schedule</h2>

          <p> TBD</p>
        </div >  
      </div > 
    </div >  
  </section>   

  <section id="call_for_papers" class="bg-light">
  </section>

  <!-- <section id="papers">
    <div class="container">
      <div class="col-lg-8 mx-auto">

      </div>
    </div>
  </section> -->

  <section id="people" class="bg-light">
    <div class="container">
      
      <div class="row">
      <div class="col-lg-8 mx-auto" style="margin-top: -300px;">
        

          <h2>Committee</h2>
          <p class="lead">Organizers</p>
          <ul>
            <li><a href="http://web.cs.ucla.edu/~kwchang/">Kai-Wei Chang</a> - UCLA, Amazon Visiting Academic</li>  
            <li><a href="https://anaeliaovalle.github.io/">Anaelia Ovalle</a> - UCLA</li>          
            <li><a href="http://ycao95.umiacs.io/">Yang Trista Cao</a> - University of Maryland</li>            
            <li><a href="http://scf.usc.edu/~ninarehm/">Ninareh Mehrabi</a> - Amazon Alexa AI</li>
            <li><a href="https://www.isi.edu/people/galstyan/about">Aram Galystan</a> - USC, Amazon Visiting Academic</li>
            <li><a href="https://jwaladhamala.com/">Jwala Dhamala</a> - Amazon Alexa AI</li>
            <li><a href="https://www.amazon.science/author/anoop-kumar">Anoop Kumar</a> - Amazon Alexa AI</li>
            <li><a href="https://guptarah.github.io/">Rahul Gupta</a> - Amazon Alexa AI</li>
          </ul>
        
          
          <p class="lead">Program Committee</p>
          <ul>
            <li>Saied Alshahrani</li>
            <li>Connor Baumler</li>
            <li>Gagan Bhatia</li>
            <li>Keith Burghardt</li>
            <li>Yang Trista Cao</li>
            <li>Javier Carnerero Cano</li>
            <li>Canyu Chen</li>
            <li>Xinyue Chen</li>
            <li>Jwala Dhamala</li>
            <li>Árdís Elíasdóttir</li>
            <li>Aram Galstyan</li>
            <li>Usman Gohar</li>
            <li>Zihao He</li>
            <li>Pengfei He</li>
            <li>Qian Hu</li>
            <li>Satyapriya Krishna</li>
            <li>Jooyoung Lee</li>
            <li>Yanan Long</li>
            <li>Subho Majumdar</li>
            <li>Ninareh Mehrabi</li>
            <li>Sahil Mishra</li>
            <li>Isar Nejadgholi</li>
            <li>Huy Nghiem</li>
            <li>Anaelia Ovalle</li>
            <li>Jieyu Zhao</li>
            <li>Aishwarya Padmakumar</li>
            <li>Kartik Perisetla</li>
            <li>Salman Rahman</li>
            <li>Chahat Raj</li>
            <li>Anthony Rios</li>
            <li>Patricia Thaine</li>
            <li>Simon Yu</li>
            <li>Xinlin Zhuang</li>
            <li>Chupeng Zhang</li>
            <li>Chenyang Zhu</li>
            <li>Christina Chance</li>            
            <li>Nishant Balepur</li>                        
            <li>Elaine Yixin Wan</li>            
            <li>Xinchen Yang</li>                        
          </ul>
          
        </div>
      </div>
    </div>

    <div class="container", style="margin-top: 50px;">
      <div class="row">
        <div class="col-lg-8 mx-auto">
                <h5>Interested in reviewing for TrustNLP?</h5>
  
                <p>
                  If you are interested in reviewing submissions, please fill out this <a href="https://forms.gle/AHf3HvWMF3FVDh9q8">form</a>.


                </p>
    
        </div>
    </div>
    </div>

  </section>



  <section id="contact" class="bg-light">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto" style="margin-top: -100px;">
                      <h2>Questions?</h2>
        
                      <p>
                        Please contact us at <a href="mailto:trustnlp24naaclworkshop@googlegroups.com">trustnlp24naaclworkshop@googlegroups.com</a>.
      
      
                      </p>
          
          </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- Footer -->
  <footer class="py-5 bg-dark">
    <div class="container">
      <p class="m-0 text-center text-white">Copyright &copy; TrustNLP 2024</p>
    </div>
    <!-- /.container -->
  </footer>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Custom JavaScript for this theme -->
  <script src="js/scrolling-nav.js"></script>

</body>

</html>


            <!-- <div>

	<ul>

<li>Griffin Adams, Columbia University </li>
<li>Stefan Arnold, FAU Erlangen-Nurnberg</li>
<li>Connor Baumler, University of Maryland</li>
<li>Keith Burghardt, USC Information Sciences Institute</li>
<li>Yang Trista Cao, University of Maryland</li>
<li>Jwala Dhamala, Amazon Alexa AI-NLU</li>
<li>Jacob Eisenstein, Google</li>
<li>Katja Filippova, Google</li>
<li>Aram Galstyan, USC Information Sciences Institute</li>
<li>Umang Gupta, University of Southern California</li>
<li>Devamanyu Hazarika, Amazon</li>
<li>Zihao He, University of Southern California</li>
<li>William Held, Georgia Tech</li>
<li>Qian Hu, Amazon.com</li>
<li>Fatemah Husain, Kuwait University</li>
<li>Anoop Kumar, Amazon</li>
<li>Sasha Luccioni, Hugging Face</li>
<li>Pranav Narayanan Venkit, Pennsylvania State University</li>
<li>Isar Nejadgholi, National Research Council Canada</li>
<li>Aishwarya Padmakumar, Amazon</li>
<li>Ashwinee Panda, Princeton University</li>
<li>Anirudh Raju, Amazon, Alexa</li>
<li>Anthony Rios, University of Texas at San Antonio</li>
<li>Robik Shrestha, RIT</li>
<li>Anna Sotnikova, University of Maryland</li>
<li>Arjun Subramonian, University of California, Los Angeles</li>
<li>Jialu Wang, University of California, Santa Cruz</li>
<li>Chhavi Yadav, UCSD</li>
<li>Kiyoon Yoo, Seoul National University</li>

	</ul>  	
		</div> -->

  

        <!-- </div>
      </div>
    </div> -->
  <!-- </div> -->


  <!-- <section id="speakers" class="bg-light">
    <div class="container" style="display: none">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <h2>Speakers</h2>
            </br>
            <img src="Diyi_Yang.jpeg" class="icons">
            <strong> <p style="text-align: center; font-size: 20px"> Diyi Yang, Assistant Professor, School of Interactive Computing, Georgia Tech </p></strong>
            <p  style="text-align: center;">Diyi Yang is an assistant professor in the School of Interactive Computing at Georgia Tech. Diyi has broad interests in NLP and Computational Social Science, including dialogue summarization, limited data learning,  hate speech and bias,  as well as responsible NLP for mental health. Her work has received multiple awards (or nominations) at EMNLP, ICWSM, SIGCHI, and CSCW. She is a Microsoft Research Faculty Fellow, a Forbes 30 under 30 in Science, an IEEE “AI 10 to Watch”, a recipient of the Intel Rising Star Faculty Award, the Samsung AI Researcher of the Year and the NSF CAREER Award.</p>

            </br>
            <img src="sbmisi.jpeg" class="icons">
            <strong> <p  style="text-align: center;">Subho Majumdar, Senior Scientist, Splunk</p> </strong>
            <p style="text-align: center;"> Subho is a senior scientist in the Applied ML Research team of Splunk. Before recently joining Splunk, he spent 3 years in AT&T Data Science and AI Research (erstwhile part of AT&T Bell Labs). Throughout his career, he has worked on data-driven solutions that pushed the boundaries for a variety of challenging and cross-product problems. His current focus is on trustworthy machine learning in the wild: not only proposing novel solutions to technical problems that ensure qualities such as fairness, transparency, privacy, and robustness, but also implementing them in real-world use cases.</p>

            </br>
            <img src="Fei_Wang.jpeg" class="icons">
            <strong> <p  style="text-align: center;">Fei Wang, Associate Professor in Division of Health Informatics, Cornell University</p> </strong>
            <p style="text-align: center;">Fei Wang is an Associate Professor in Division of Health Informatics, Department of Population Health Sciences, Weill Cornell Medicine, Cornell University. His major research interest is data mining, machine learning and their applications in health data science. He has published more than 250 papers on the top venues of related areas such as ICML, KDD, NIPS, CVPR, AAAI, IJCAI, JAMA Internal Medicine, Annals of Internal Medicine, Lancet Digital Health, etc. His papers have received over 19,000 citations so far with an H-index 67. His (or his students’) papers have won 8 best paper (or nomination) awards at top international conferences on data mining and medical informatics. His team won the championship of the NIPS/Kaggle Challenge on Classification of Clinically Actionable Genetic Mutations in 2017 and Parkinson's Progression Markers' Initiative data challenge organized by Michael J. Fox Foundation in 2016. Dr. Wang is the recipient of the NSF CAREER Award in 2018, as well as the inaugural research leadership award in IEEE International Conference on Health Informatics (ICHI) 2019. Dr. Wang’s Research has been supported by NSF, NIH, ONR, PCORI, MJFF, AHA, Amazon, etc. Dr. Wang is the past chair of the Knowledge Discovery and Data Mining working group in American Medical Informatics Association (AMIA). Dr. Wang is a fellow of AMIA and a Distinguished Member of ACM.</p>
        </div>
      </div>
    </div>
  </section> -->



